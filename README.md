# Q-and-A-bot-using-llama3

This project aims to develop a Q&A bot designed to answer questions based on article that details creating a model capable of providing question-answering capabilities for genuine tweets used by journalists to write news articles. The project uses vector embeddings with Ollama embedding to store data in a Chroma vector store. The data is stored in chunks with some overlap between them. The model used to create the bot is LLaMA3 by Meta. Subsequently, RAG (Retrieval-Augmented Generation) is implemented using a prompt template that guides the model to answer questions accurately based on the provided context. Finally, the model and the prompt used to develop the chai, along with retrieval to interact with the vector store, generate the response.
